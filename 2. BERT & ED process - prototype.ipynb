{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requisitos**\n",
    "- modelo (h5)\n",
    "- labeled data (training and testing sentences *.p)\n",
    "- NYT corpus (*.metadata *.text *.lemmas.p)\n",
    "- outputfolder (where to store the *.events.p)\n",
    "- modules.ed ( the ed.py script where to recover the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 11:46:24.396477 [WARNING] Bad or missing parameters. Defined to default:\n",
      "2020-06-12 11:46:24.396638 [ INFO  ] Using /mnt/work/maiso/datasets/NYT as the root data path.\n",
      "2020-06-12 11:46:24.396764 [ INFO  ] Using /mnt/work/maiso/datasets/output as the destination folder.\n",
      "\n",
      "2020-06-12 11:46:24.396905 [ INFO  ] Using /mnt/work/maiso/python3.workspace/Causality/data/ed/ as the root labeled data path.\n",
      "2020-06-12 11:46:24.397024 [ INFO  ] Using /mnt/work/maiso/python3.workspace/Causality/models/best_model_15.h5 as the model data path.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sys, getopt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "argv = sys.argv[1:]\n",
    "try:\n",
    "    opts, args = getopt.getopt(argv,\"\",[\"data_path=\",\n",
    "                                        \"destination_path=\",\n",
    "                                        \"labeled_data_path=\",\n",
    "                                        \"model_path=\"\n",
    "                                       ])\n",
    "    \n",
    "    for opt, arg in opts:\n",
    "        if opt == \"--data_path\":\n",
    "            data_path = arg\n",
    "        if opt == \"--destination_path\":\n",
    "            destination_path = arg\n",
    "        if opt == \"--labeled_data_path\":\n",
    "            labeled_data_path = arg\n",
    "        if opt == \"--model_path\":\n",
    "            model_path = arg\n",
    "\n",
    "            \n",
    "            \n",
    "except getopt.GetoptError:\n",
    "\n",
    "    print(f'{datetime.now()} [WARNING] Bad or missing parameters. Defined to default:')\n",
    "\n",
    "    \n",
    "    # Data location\n",
    "    # data root where search for the *.metadata, *.text, *.lemmas.p. \n",
    "    data_path = '/mnt/work/maiso/datasets/NYT'\n",
    "    # data folder where put the *.events.p files\n",
    "    destination_path = '/mnt/work/maiso/datasets/output'\n",
    "    \n",
    "    labeled_data_path='/mnt/work/maiso/python3.workspace/Causality/data/ed/'\n",
    "    model_path = '/mnt/work/maiso/python3.workspace/Causality/models/best_model_15.h5'\n",
    "    \n",
    "print(f'{datetime.now()} [ INFO  ] Using {data_path} as the root data path.')\n",
    "print(f'{datetime.now()} [ INFO  ] Using {destination_path} as the destination folder.')\n",
    "print()\n",
    "print(f'{datetime.now()} [ INFO  ] Using {labeled_data_path} as the root labeled data path.')\n",
    "print(f'{datetime.now()} [ INFO  ] Using {model_path} as the model data path.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 11:46:25.877262 [ INFO  ] Finished imports, starting program.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#BERT\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import  BertModel,BertForMaskedLM\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import threading\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Finished imports, starting program.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 11:46:28.431876 [ INFO  ] Reading *.metadata filenames.\n",
      "2020-06-12 11:46:40.120771 [ INFO  ] Finished reading *.metadata filenames from directory. Working with 1855658 files.\n"
     ]
    }
   ],
   "source": [
    "print(f'{datetime.now()} [ INFO  ] Reading *.metadata filenames.')\n",
    "\n",
    "## 1. Built the list with all the filenames:\n",
    "files = []\n",
    "for dirpath, _, filenames in os.walk(data_path):\n",
    "    filenames = [filename for filename in filenames if filename.endswith('metadata')]\n",
    "    for filename in filenames:\n",
    "        file = os.path.join(dirpath, filename)\n",
    "        files.append(file)\n",
    "\n",
    "files.sort()\n",
    "print(f'{datetime.now()} [ INFO  ] Finished reading *.metadata filenames from directory. Working with {len(files)} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 11:44:41.477498 [ INFO  ] Curently are 1855658 XMLs, 1855658 metadatas,1855658 texts, 1855658 lemmas and 0 event files.\n"
     ]
    }
   ],
   "source": [
    "xml_count = 0\n",
    "metadata_count = 0\n",
    "text_count = 0\n",
    "lemmas_count = 0\n",
    "events_count = 0\n",
    "for dirpath, _, filenames in os.walk(data_path):\n",
    "    xml_count += len([filename for filename in filenames if filename.endswith('.xml')])\n",
    "    metadata_count += len([filename for filename in filenames if filename.endswith('metadata')])\n",
    "    text_count += len([filename for filename in filenames if filename.endswith('.text')])\n",
    "    lemmas_count += len([filename for filename in filenames if filename.endswith('.lemmas.p')])\n",
    "    events_count += len([filename for filename in filenames if filename.endswith('.events.p')])\n",
    "print(f'{datetime.now()} [ INFO  ] Curently are {xml_count} XMLs, {metadata_count} metadatas,{text_count} texts, {lemmas_count} lemmas and {events_count} event files.')\n",
    "\n",
    "if events_count>0:\n",
    "    print(f'{datetime.now()} [WARNING] Some files are goint to be overwritten (*.events.p)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 11:46:52.582511 [ INFO  ] Loading BERT class\n",
      "2020-06-12 11:46:52.583205 [ INFO  ] BERT class loaded.\n"
     ]
    }
   ],
   "source": [
    "print(f'{datetime.now()} [ INFO  ] Loading BERT class')\n",
    "class BERT_Embeddings(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        # Load pre-trained model (weights)\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def get_BERT_Embeddings(self,text, tokens):\n",
    "\n",
    "            \n",
    "        matrix = np.zeros(shape=(1,len(tokens),768)) # 768 last 4 layer added.\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        tokenized_text = ['[CLS]']\n",
    "        \n",
    "        for idx, (ini,fin) in enumerate(tokens):\n",
    "            word_tokens = self.tokenizer.tokenize(text[ini:fin])\n",
    "\n",
    "            if len(word_tokens)+len(tokenized_text)+1 >512: #if sentence too long exclude some tokens\n",
    "                break \n",
    "                \n",
    "            tokenized_text += word_tokens\n",
    "        \n",
    "        # truncate sentence if is longer than 512 (BERT model can't handle more than 512)\n",
    "        # if sentence too long exclude some tokens (the vectors of those tokens will be zero)\n",
    "        tokens = tokens[:idx]\n",
    "\n",
    "            \n",
    "        tokenized_text += ['[SEP]']\n",
    "\n",
    "            \n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n",
    "            \n",
    "        token_embeddings = [] \n",
    "        layer_i = 0\n",
    "        batch_i = 0\n",
    "        token_i = 0\n",
    "\n",
    "        # For each token in the sentence...\n",
    "        for token_i in range(len(tokenized_text)):\n",
    "            hidden_layers = [] \n",
    "\n",
    "            # For each of the 12 layers...\n",
    "            for layer_i in range(len(encoded_layers)):\n",
    "\n",
    "                # Lookup the vector for `token_i` in `layer_i`\n",
    "                vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "                hidden_layers.append(vec)\n",
    "\n",
    "            token_embeddings.append(hidden_layers)\n",
    "\n",
    "        summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings] # [number_of_tokens, 768]\n",
    "\n",
    "        inicio=1\n",
    "        for idx,token in enumerate(tokens):\n",
    "                ini,fin = token\n",
    "                individual_tokenized = self.tokenizer.tokenize(text[ini:fin])\n",
    "                cantidad_embeddings = len(individual_tokenized)\n",
    "                group_tokenized = tokenized_text[inicio:inicio+cantidad_embeddings]\n",
    "                                \n",
    "                assert all([t1==t2 for t1,t2 in zip(individual_tokenized,group_tokenized)]),  '{} {} {}'.format(\n",
    "                                 text[ini:fin],\n",
    "                                 individual_tokenized,\n",
    "                                 group_tokenized\n",
    "                             )\n",
    "\n",
    "                if cantidad_embeddings>0:\n",
    "                    for embedding in summed_last_4_layers[inicio:inicio+cantidad_embeddings]:\n",
    "                        matrix[0,idx,:]+=embedding.numpy()\n",
    "                    matrix[0,idx,:] = matrix[0,idx,:]/cantidad_embeddings\n",
    "                    inicio+=cantidad_embeddings\n",
    "\n",
    "        return matrix\n",
    "print(f'{datetime.now()} [ INFO  ] BERT class loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need the entity vocab for the next step. We first load model and vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    from modules.ed import get_data, load_model_from_disk, Generator, train_val_split, train_model, get_model, sensitivity, specificity, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 11:47:01.294400 [ INFO  ] Loading data from files (testing_sents.p and training_sents.p) on folder: /mnt/work/maiso/python3.workspace/Causality/data/ed/\n",
      "[  OK   ] Loading training indexes.\n",
      "2020-06-12 11:47:54.250779 [ INFO  ] Loading model from: /mnt/work/maiso/python3.workspace/Causality/models/best_model_15.h5\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2020-06-12 11:47:55.152539 [ INFO  ] evaluating model on validation_data\n",
      "loss: 0.1729\n",
      "acc: 0.9389\n",
      "sensitivity: 0.6760\n",
      "specificity: 0.9439\n",
      "f1_score: 0.6854\n"
     ]
    }
   ],
   "source": [
    "#from modules.ed import get_data, load_model_from_disk, Generator, train_val_split, train_model, get_model, sensitivity, specificity, f1_score\n",
    "\n",
    "\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Loading data from files (testing_sents.p and training_sents.p) on folder: {labeled_data_path}')\n",
    "      \n",
    "    \n",
    "x, y, ent2index = get_data(labeled_data_path)\n",
    "train_data, val_data = train_val_split((x,y))\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Loading model from: {model_path}')\n",
    "# model = load_model_from_disk(model_path)\n",
    "model = get_model(len(ent2index), hidden_units_count=15)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "          optimizer='adam', \n",
    "          metrics=['accuracy',sensitivity,specificity, f1_score])\n",
    "\n",
    "model.load_weights(model_path)\n",
    "\n",
    "model._make_predict_function() # para read-only y evitar problemas de sincronización con multithreading\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] evaluating model on validation_data')\n",
    "x_val, y_val = val_data\n",
    "rta = model.evaluate(Generator(list(zip(x_val, y_val))), verbose=0)\n",
    "\n",
    "# for metric, value in zip(['loss']+[m.name for m in model.metrics], rta):\n",
    "#     print(f'{datetime.now()} [ INFO  ] {metric:12}: {value}')\n",
    "for metric,value in zip(model.metrics_names, rta):\n",
    "    print('{}: {:5.4f}'.format(metric, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 11:48:22.750111 [ INFO  ] Loading BERT model.\n",
      "2020-06-12 11:48:22.750377 [ INFO  ] BERT model loaded.\n",
      "2020-06-12 11:48:22.751126 [ INFO  ] process_file defined. Uses: metadata, lemmas and text for building the event.p files.\n"
     ]
    }
   ],
   "source": [
    "# Inputs: \n",
    "bert = BERT_Embeddings()\n",
    "# bert_lock = threading.Lock()\n",
    "\n",
    "    \n",
    "# model_lock = threading.Lock()\n",
    "tuplas_re = re.compile('\\(([0-9]*?), ([0-9]*?)\\)')\n",
    "entities_re = re.compile(\"'([^']*)'\")\n",
    "path_re = re.compile('([0-9]{4}/[0-9]{2}/[0-9]{2}/.*)\\.metadata')\n",
    "\n",
    "#Matches any character which is not a word character.\n",
    "words_re = re.compile('\\w')\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Loading BERT model.')\n",
    "print(f'{datetime.now()} [ INFO  ] BERT model loaded.')\n",
    "\n",
    "def process_file(file):\n",
    "\n",
    "\n",
    "\n",
    "    # read file (metadata + text)\n",
    "    text = open(file[:-9]+'.text', 'r' ).read()\n",
    "    lemmas = pickle.load(open(file[:-9]+'.lemmas.p', 'rb' ))\n",
    "\n",
    "    metadata = open(file,'r').read().splitlines()\n",
    "    id_ = metadata[0][4:] #len('ID: ') == 4\n",
    "    date = metadata[1][6:] # len('DATE: ') == 6\n",
    "\n",
    "    # process file\n",
    "    tokens = tuplas_re.findall(metadata[3])\n",
    "    sentences = tuplas_re.findall(metadata[2])\n",
    "\n",
    "    tokens = [(int(ini),int(fin)) for ini, fin in tokens]\n",
    "    sentences = [(int(ini),int(fin)) for ini, fin in sentences]\n",
    "\n",
    "    entities = entities_re.findall(metadata[4])\n",
    "\n",
    "    idx = 0\n",
    "    old_idx = 0\n",
    "    sent_idx=0\n",
    "    events = []\n",
    "\n",
    "    for ini,fin in sentences:\n",
    "        while idx<len(tokens) and tokens[idx][0]<fin:\n",
    "            idx+=1\n",
    "        tokens_in_sentence = tokens[old_idx:idx]\n",
    "        lemmas_in_sentence = lemmas[old_idx:idx]\n",
    "        entities_in_sentence = entities[old_idx:idx]\n",
    "        old_idx = idx\n",
    "\n",
    "\n",
    "        xbert = bert.get_BERT_Embeddings(text, tokens_in_sentence )\n",
    "\n",
    "        xsentbert = np.average(xbert, axis=1) *np.ones(shape=(1,len(xbert[0,:,0]),768))\n",
    "        xents = np.zeros(shape=(1,len(tokens_in_sentence)))\n",
    "\n",
    "\n",
    "        for idx,entity in enumerate(entities_in_sentence):\n",
    "            if entity in ent2index:\n",
    "                xents[0,idx] = ent2index[entity]\n",
    "            else:\n",
    "                xents[0,idx] = ent2index['[PAD]']\n",
    "\n",
    "        x = [xbert, xsentbert, xents]\n",
    "\n",
    "        y_pred = model.predict(x)\n",
    "\n",
    "        for j in range(len(tokens_in_sentence)):\n",
    "            pred = y_pred[0,j,0]\n",
    "            token = tokens_in_sentence[j]\n",
    "\n",
    "            if pred>0.5 and len(words_re.findall(text[token[0]:token[1]]))>0: # event with some word/letter\n",
    "                bert_vec = xbert[0,j,:]\n",
    "\n",
    "\n",
    "                event = {\n",
    "                    'file_id':id_,\n",
    "                    'trigger': text[token[0]:token[1]],\n",
    "                    'token': (token[0],token[1]),\n",
    "                    'idx': sent_idx,\n",
    "#                     'bert': bert_vec,\n",
    "                    'conf': pred,\n",
    "                    'lemma': lemmas_in_sentence[j],\n",
    "                    'date': date\n",
    "                }\n",
    "                events.append(event)\n",
    "                \n",
    "        sent_idx+=1\n",
    "\n",
    "    # extracting YYYY/MM/DD/file_name (without .metadatata)\n",
    "    # joinint ( destination_path + YYYY/MM/DDfile_name + .events.p)\n",
    "    event_file = os.path.join(destination_path, path_re.findall(file)[0]+'.events.p')\n",
    "    \n",
    "    # creating destination_path + YYYY/MM/DD folders\n",
    "    folder = '/'.join(event_file.split('/')[:-1])\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "\n",
    "    pickle.dump(events ,open(event_file, 'wb' ))\n",
    "    return events\n",
    "\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] process_file defined. Uses: metadata, lemmas and text for building the event.p files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-10 16:26:40.008289 [ INFO  ] Starting the concurrent processing of all the files (1804358) using max_workers=16 and chunksize=112772.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# files = files[:2000]\n",
    "# print(f'{datetime.now()} [WARNING]  Se están usando menos archivos.')\n",
    "\n",
    "max_workers=16 \n",
    "chunksize = int(len(files)/(max_workers))\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Starting the concurrent processing of all the files ({len(files)}) using max_workers={max_workers} and chunksize={chunksize}.')\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    list(executor.map(process_file, files, chunksize=chunksize))\n",
    "    \n",
    "print(f'{datetime.now()} [  OK   ] Finished!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
