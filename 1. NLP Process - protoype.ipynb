{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-02 21:23:15.084981 [INFO] Finished imports, starting program.\n",
      "2020-06-02 21:23:15.085123 [INFO] Using /mnt/work/maiso/datasets/NYT as the root data path.\n",
      "2020-06-02 21:23:15.085234 [INFO] Running with 16 no. of process.\n",
      "2020-06-02 21:23:15.085306 [INFO] The following program create the files: *.metadata, *.text and *.lemmas.p\n",
      "2020-06-02 21:23:20.845944 [INFO] Curently are 1855658 XMLs, 13689 metadatas,1855658 texts and 13688 lemmas\n",
      "2020-06-02 21:23:20.846190 [WARN] Some files are goint to be overwritten\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "print(f'{datetime.now()} [ INFO  ] Finished imports, starting program.')\n",
    "\n",
    "# Data location\n",
    "data_path = '/mnt/work/maiso/datasets/NYT'\n",
    "print(f'{datetime.now()} [ INFO  ] Using {data_path} as the root data path.')\n",
    "# Nro of Process defined to be equal to nro of cores.\n",
    "nro_of_process = 16\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Running with {nro_of_process} no. of process.')\n",
    "\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] The following program create the files: *.metadata, *.text and *.lemmas.p')\n",
    "\n",
    "xml_count = 0\n",
    "metadata_count = 0\n",
    "text_count = 0\n",
    "lemmas_count = 0\n",
    "for dirpath, _, filenames in os.walk(data_path):\n",
    "    xml_count += len([filename for filename in filenames if filename.endswith('.xml')])\n",
    "    metadata_count += len([filename for filename in filenames if filename.endswith('metadata')])\n",
    "    text_count += len([filename for filename in filenames if filename.endswith('.text')])\n",
    "    lemmas_count += len([filename for filename in filenames if filename.endswith('.lemmas.p')])\n",
    "print(f'{datetime.now()} [ INFO  ] Curently are {xml_count} XMLs, {metadata_count} metadatas,{text_count} texts and {lemmas_count} lemmas')\n",
    "\n",
    "if text_count>0 or lemmas_count>0 or metadata_count>0:\n",
    "    print(f'{datetime.now()} [WARNING] Some files are goint to be overwritten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-02 21:21:29.455866 [INFO] Reading .xml filenames.\n",
      "2020-06-02 21:21:37.485527 [INFO] Finished reading .xml filenames from directory. Working with 1855658 files.\n"
     ]
    }
   ],
   "source": [
    "## 1. Built the list with all the filenames:\n",
    "print(f'{datetime.now()} [ INFO  ] Reading .xml filenames.')\n",
    "files = []\n",
    "for dirpath, _, filenames in os.walk(data_path):\n",
    "    filenames = [filename for filename in filenames if filename.endswith('xml')]\n",
    "    for filename in filenames:\n",
    "        file = os.path.join(dirpath, filename)\n",
    "        files.append(file)\n",
    "\n",
    "files.sort()\n",
    "print(f'{datetime.now()} [ INFO  ] Finished reading .xml filenames from directory. Working with {len(files)} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-02 21:24:40.047177 [INFO] Compiling regex patterns and defining generator of articles\n",
      "2020-06-02 21:24:40.962237 [INFO] Starting multiprocessing with chunksize=38659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-13:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "print(f'{datetime.now()} [ INFO  ] Compiling regex patterns and defining generator of articles')\n",
    "article_text_ =  re.compile('<block class=\"full_text\".*?>(.*?)</block>',re.DOTALL)\n",
    "article_date_ = re.compile('<pubdata date.publication=\"([0-9]{8})T[0-9]{6}\".*?/>', re.DOTALL)\n",
    "article_id_ = re.compile('<doc-id id-string=\"([0-9]{1,7})\"/>')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['textcat'])\n",
    "\n",
    "\n",
    "articles = (open(file,'r').read() for file in files)\n",
    "\n",
    "\n",
    "\n",
    "def NLPProcess(article):\n",
    "    text = article_text_.findall(article)\n",
    "    text = text[0] if len(text)==1 else ''\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    spacy_doc = nlp(text)\n",
    "    \n",
    "    date = article_date_.findall(article)[0]\n",
    "    id_ = article_id_.findall(article)\n",
    "\n",
    "    assert len(id_) == 1, 'The article has more than one identifier'\n",
    "    id_ = f'{int(id_[0]):07}'\n",
    "\n",
    "    writer = open(f'{data_path}/{date[:4]}/{date[4:6]}/{date[6:8]}/{id_}.metadata', 'w')\n",
    "    \n",
    "    # tokens\n",
    "    tokens = []\n",
    "    if len(spacy_doc)!= 0:\n",
    "        tokens = [(token.idx, token.idx+len(token.text)) for token in spacy_doc]\n",
    "    \n",
    "    # sents\n",
    "    sents = []\n",
    "    if len(spacy_doc)!= 0:\n",
    "        inis  = [spacy_doc[sent.start].idx for sent in spacy_doc.sents]\n",
    "        fins  = inis[1:]+[spacy_doc[-1].idx+len(spacy_doc[-1].text)]\n",
    "        sents = list(zip(inis,fins))\n",
    "    \n",
    "    # entities\n",
    "    entities = []\n",
    "    if len(spacy_doc)!= 0:\n",
    "        #entities = [token for token in spacy_doc if token.ent_iob_!='O']\n",
    "        entities = [token for token in spacy_doc]\n",
    "        entities = [ f'{token.ent_iob_}-{token.ent_type_}' for token in entities]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(spacy_doc)!= 0:\n",
    "        lemmas = [token.lemma_ for token in spacy_doc]\n",
    "    else:\n",
    "        lemmas = []\n",
    "        \n",
    "    writer.write(f'ID: {id_}\\n')\n",
    "    writer.write(f'DATE: {date}\\n')\n",
    "    writer.write(f'SENTS: {sents}\\n')\n",
    "    writer.write(f'TOKEN: {tokens}\\n')\n",
    "    writer.write(f'ENTITIES: {entities}\\n')\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    \n",
    "    writer = open(f'{data_path}/{date[:4]}/{date[4:6]}/{date[6:8]}/{id_}.text', 'w')\n",
    "    writer.write(text)\n",
    "    writer.close()\n",
    "    \n",
    "    pickle.dump(lemmas, open(f'{data_path}/{date[:4]}/{date[4:6]}/{date[6:8]}/{id_}.lemmas.p', 'wb'))\n",
    "\n",
    "    \n",
    "\n",
    "chunksize = int(len(files)/(3*nro_of_process)) # ~3 chunks per process.\n",
    "print(f'{datetime.now()} [ INFO  ] Starting multiprocessing with chunksize={chunksize}')\n",
    "with Pool(processes=nro_of_process) as pool:\n",
    "    pool.map(NLPProcess, articles,chunksize=chunksize)\n",
    "    \n",
    "print(f'{datetime.now()} [  OK   ] Finished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
