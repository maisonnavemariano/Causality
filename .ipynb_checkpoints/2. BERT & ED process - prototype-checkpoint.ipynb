{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-07 18:18:16.909171 [ INFO  ] Finished imports, starting program.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "#BERT\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import  BertModel,BertForMaskedLM\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import threading\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Finished imports, starting program.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-07 18:18:16.915458 [ INFO  ] Using /mnt/work/maiso/datasets/NYT as the root data path.\n"
     ]
    }
   ],
   "source": [
    "# Data location\n",
    "data_path = '/mnt/work/maiso/datasets/NYT'\n",
    "print(f'{datetime.now()} [ INFO  ] Using {data_path} as the root data path.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-07 18:18:17.127887 [ INFO  ] Reading *.metadata filenames.\n",
      "2020-06-07 18:18:30.343071 [ INFO  ] Finished reading *.metadata filenames from directory. Working with 1855658 files.\n"
     ]
    }
   ],
   "source": [
    "print(f'{datetime.now()} [ INFO  ] Reading *.metadata filenames.')\n",
    "\n",
    "## 1. Built the list with all the filenames:\n",
    "files = []\n",
    "for dirpath, _, filenames in os.walk(data_path):\n",
    "    filenames = [filename for filename in filenames if filename.endswith('metadata')]\n",
    "    for filename in filenames:\n",
    "        file = os.path.join(dirpath, filename)\n",
    "        files.append(file)\n",
    "\n",
    "files.sort()\n",
    "print(f'{datetime.now()} [ INFO  ] Finished reading *.metadata filenames from directory. Working with {len(files)} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-07 18:19:00.988007 [ INFO  ] Curently are 1855658 XMLs, 1855658 metadatas,1855658 texts, 1855658 lemmas and 812 event files.\n",
      "2020-06-07 18:19:00.988283 [WARNING] Some files are goint to be overwritten (*.events.p)\n"
     ]
    }
   ],
   "source": [
    "xml_count = 0\n",
    "metadata_count = 0\n",
    "text_count = 0\n",
    "lemmas_count = 0\n",
    "events_count = 0\n",
    "for dirpath, _, filenames in os.walk(data_path):\n",
    "    xml_count += len([filename for filename in filenames if filename.endswith('.xml')])\n",
    "    metadata_count += len([filename for filename in filenames if filename.endswith('metadata')])\n",
    "    text_count += len([filename for filename in filenames if filename.endswith('.text')])\n",
    "    lemmas_count += len([filename for filename in filenames if filename.endswith('.lemmas.p')])\n",
    "    events_count += len([filename for filename in filenames if filename.endswith('.events.p')])\n",
    "print(f'{datetime.now()} [ INFO  ] Curently are {xml_count} XMLs, {metadata_count} metadatas,{text_count} texts, {lemmas_count} lemmas and {events_count} event files.')\n",
    "\n",
    "if events_count>0:\n",
    "    print(f'{datetime.now()} [WARNING] Some files are goint to be overwritten (*.events.p)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-07 18:19:11.214615 [ INFO  ] Loading BERT class\n",
      "2020-06-07 18:19:11.215362 [ INFO  ] BERT class loaded.\n"
     ]
    }
   ],
   "source": [
    "print(f'{datetime.now()} [ INFO  ] Loading BERT class')\n",
    "class BERT_Embeddings(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        # Load pre-trained model (weights)\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def get_BERT_Embeddings(self,text, tokens):\n",
    "        matrix = np.zeros(shape=(1,len(tokens),768)) # 768 last 4 layer added.\n",
    "\n",
    "        tokenized_text = ['[CLS]']\n",
    "        \n",
    "        for ini,fin in tokens:\n",
    "            word_tokens = self.tokenizer.tokenize(text[ini:fin])\n",
    "            tokenized_text +=word_tokens\n",
    "        tokenized_text += ['[SEP]']\n",
    "        \n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n",
    "            \n",
    "        token_embeddings = [] \n",
    "        layer_i = 0\n",
    "        batch_i = 0\n",
    "        token_i = 0\n",
    "\n",
    "        # For each token in the sentence...\n",
    "        for token_i in range(len(tokenized_text)):\n",
    "            hidden_layers = [] \n",
    "\n",
    "            # For each of the 12 layers...\n",
    "            for layer_i in range(len(encoded_layers)):\n",
    "\n",
    "                # Lookup the vector for `token_i` in `layer_i`\n",
    "                vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "                hidden_layers.append(vec)\n",
    "\n",
    "            token_embeddings.append(hidden_layers)\n",
    "\n",
    "        summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings] # [number_of_tokens, 768]\n",
    "\n",
    "        inicio=1\n",
    "        for idx,token in enumerate(tokens):\n",
    "                ini,fin = token\n",
    "                individual_tokenized = self.tokenizer.tokenize(text[ini:fin])\n",
    "                cantidad_embeddings = len(individual_tokenized)\n",
    "                group_tokenized = tokenized_text[inicio:inicio+cantidad_embeddings]\n",
    "                                \n",
    "                assert all([t1==t2 for t1,t2 in zip(individual_tokenized,group_tokenized)]),  '{} {} {}'.format(\n",
    "                                 text[ini:fin],\n",
    "                                 individual_tokenized,\n",
    "                                 group_tokenized\n",
    "                             )\n",
    "\n",
    "                if cantidad_embeddings>0:\n",
    "                    for embedding in summed_last_4_layers[inicio:inicio+cantidad_embeddings]:\n",
    "                        matrix[0,idx,:]+=embedding.numpy()\n",
    "                    matrix[0,idx,:] = matrix[0,idx,:]/cantidad_embeddings\n",
    "                    inicio+=cantidad_embeddings\n",
    "\n",
    "        return matrix\n",
    "print(f'{datetime.now()} [ INFO  ] BERT class loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need the entity vocab for the next step. We first load model and vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.ed import get_data, load_model_from_disk, Generator, train_val_split, train_model, get_model, sensitivity, specificity, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-07 18:19:30.257204 [ INFO  ] Loading data from files (testing_sents.p and training_sents.p) on folder: /mnt/work/maiso/python3.workspace/Causality/data/ed/\n",
      "[  OK   ] Loading training indexes.\n",
      "2020-06-07 18:20:56.323769 [ INFO  ] Loading model from: /mnt/work/maiso/python3.workspace/Causality/models/best_model_15.h5\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2020-06-07 18:20:57.232169 [ INFO  ] evaluating model on validation_data\n",
      "loss: 0.1671\n",
      "acc: 0.9375\n",
      "sensitivity: 0.6853\n",
      "specificity: 0.9416\n",
      "f1_score: 0.6897\n"
     ]
    }
   ],
   "source": [
    "from modules.ed import get_data, load_model_from_disk, Generator, train_val_split, train_model, get_model, sensitivity, specificity, f1_score\n",
    "\n",
    "data_path='/mnt/work/maiso/python3.workspace/Causality/data/ed/'\n",
    "model_path = '/mnt/work/maiso/python3.workspace/Causality/models/best_model_15.h5'\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Loading data from files (testing_sents.p and training_sents.p) on folder: {data_path}')\n",
    "      \n",
    "    \n",
    "x, y, ent2index = get_data(data_path)\n",
    "train_data, val_data = train_val_split((x,y))\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Loading model from: {model_path}')\n",
    "# model = load_model_from_disk(model_path)\n",
    "model = get_model(len(ent2index), hidden_units_count=15)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "          optimizer='adam', \n",
    "          metrics=['accuracy',sensitivity,specificity, f1_score])\n",
    "\n",
    "model.load_weights(model_path)\n",
    "\n",
    "model._make_predict_function() # para read-only y evitar problemas de sincronización con multithreading\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] evaluating model on validation_data')\n",
    "x_val, y_val = val_data\n",
    "rta = model.evaluate(Generator(list(zip(x_val, y_val))), verbose=0)\n",
    "\n",
    "# for metric, value in zip(['loss']+[m.name for m in model.metrics], rta):\n",
    "#     print(f'{datetime.now()} [ INFO  ] {metric:12}: {value}')\n",
    "for metric,value in zip(model.metrics_names, rta):\n",
    "    print('{}: {:5.4f}'.format(metric, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-05 22:18:01.385737 [ INFO  ] Loading BERT model.\n",
      "2020-06-05 22:18:01.386658 [ INFO  ] BERT model loaded.\n",
      "2020-06-05 22:18:01.387972 [ INFO  ] process_file defined. Uses: metadata, lemmas and text for building the event.p files.\n"
     ]
    }
   ],
   "source": [
    "# Inputs: \n",
    "bert = BERT_Embeddings()\n",
    "# bert_lock = threading.Lock()\n",
    "\n",
    "    \n",
    "# model_lock = threading.Lock()\n",
    "tuplas_re = re.compile('\\(([0-9]*?), ([0-9]*?)\\)')\n",
    "entities_re = re.compile(\"'([^']*)'\")\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Loading BERT model.')\n",
    "print(f'{datetime.now()} [ INFO  ] BERT model loaded.')\n",
    "\n",
    "def process_file(file):\n",
    "\n",
    "\n",
    "\n",
    "    # read file (metadata + text)\n",
    "    text = open(file[:-9]+'.text', 'r' ).read()\n",
    "    lemmas = pickle.load(open(file[:-9]+'.lemmas.p', 'rb' ))\n",
    "\n",
    "    metadata = open(file,'r').read().splitlines()\n",
    "    id_ = metadata[0][4:] #len('ID: ') == 4\n",
    "    date = metadata[1][6:] # len('DATE: ') == 6\n",
    "\n",
    "    # process file\n",
    "    tokens = tuplas_re.findall(metadata[3])\n",
    "    sentences = tuplas_re.findall(metadata[2])\n",
    "\n",
    "    tokens = [(int(ini),int(fin)) for ini, fin in tokens]\n",
    "    sentences = [(int(ini),int(fin)) for ini, fin in sentences]\n",
    "\n",
    "    entities = entities_re.findall(metadata[4])\n",
    "\n",
    "    idx = 0\n",
    "    old_idx = 0\n",
    "    sent_idx=0\n",
    "    for ini,fin in sentences:\n",
    "        while idx<len(tokens) and tokens[idx][0]<fin:\n",
    "            idx+=1\n",
    "        tokens_in_sentence = tokens[old_idx:idx]\n",
    "        lemmas_in_sentence = lemmas[old_idx:idx]\n",
    "        entities_in_sentence = entities[old_idx:idx]\n",
    "        old_idx = idx\n",
    "\n",
    "#         with bert_lock:\n",
    "        xbert = bert.get_BERT_Embeddings(text, tokens_in_sentence )\n",
    "\n",
    "        xsentbert = np.average(xbert, axis=1) *np.ones(shape=(1,len(xbert[0,:,0]),768))\n",
    "        xents = np.zeros(shape=(1,len(tokens_in_sentence)))\n",
    "\n",
    "\n",
    "        for idx,entity in enumerate(entities_in_sentence):\n",
    "            if entity in ent2index:\n",
    "                xents[0,idx] = ent2index[entity]\n",
    "            else:\n",
    "                xents[0,idx] = ent2index['[PAD]']\n",
    "\n",
    "        x = [xbert, xsentbert, xents]\n",
    "#         with model_lock:\n",
    "        y_pred = model.predict(x)\n",
    "        events = []\n",
    "\n",
    "        for j in range(len(tokens_in_sentence)):\n",
    "            pred = y_pred[0,j,0]\n",
    "            if pred>0.5:\n",
    "                bert_vec = xbert[0,j,:]\n",
    "                token = tokens_in_sentence[j]\n",
    "\n",
    "\n",
    "                event = {\n",
    "                    'file_id':id_,\n",
    "                    'trigger': text[token[0]:token[1]],\n",
    "                    'idx': sent_idx,\n",
    "                    'bert': bert_vec,\n",
    "                    'conf': pred,\n",
    "                    'lemma': lemmas_in_sentence[j],\n",
    "                    'date': date\n",
    "                }\n",
    "                events.append(event)\n",
    "        sent_idx+=1\n",
    "\n",
    "        pickle.dump(events ,open(file[:-9]+'.events.p', 'wb' ))\n",
    "    return file\n",
    "\n",
    "\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] process_file defined. Uses: metadata, lemmas and text for building the event.p files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-05 22:28:57.118955 [ INFO  ] Starting the concurrent processing of all the files (1855658) using max_workers=16 and chunksize=115978.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "max_workers=16 \n",
    "chunksize = int(len(files)/(max_workers))\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Starting the concurrent processing of all the files ({len(files)}) using max_workers={max_workers} and chunksize={chunksize}.')\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    list(executor.map(process_file, files[:1000], chunksize=chunksize))\n",
    "    \n",
    "print(f'{datetime.now()} [  OK   ] Finished!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
