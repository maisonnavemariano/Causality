{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-11 13:59:14.866047 [ INFO  ] Finished imports, starting program.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "#BERT\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert import  BertModel,BertForMaskedLM\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import threading\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Finished imports, starting program.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-11 13:59:15.993690 [ INFO  ] Using /mnt/work/maiso/datasets/NYT as the root data path.\n"
     ]
    }
   ],
   "source": [
    "# Data location\n",
    "data_path = '/mnt/work/maiso/datasets/NYT'\n",
    "print(f'{datetime.now()} [ INFO  ] Using {data_path} as the root data path.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-11 13:59:16.745698 [ INFO  ] Reading *.metadata filenames.\n",
      "2020-06-11 13:59:31.224261 [ INFO  ] Finished reading *.metadata filenames from directory. Working with 1855658 files.\n"
     ]
    }
   ],
   "source": [
    "print(f'{datetime.now()} [ INFO  ] Reading *.metadata filenames.')\n",
    "\n",
    "## 1. Built the list with all the filenames:\n",
    "files = []\n",
    "for dirpath, _, filenames in os.walk(data_path):\n",
    "    filenames = [filename for filename in filenames if filename.endswith('metadata')]\n",
    "    for filename in filenames:\n",
    "        file = os.path.join(dirpath, filename)\n",
    "        files.append(file)\n",
    "\n",
    "files.sort()\n",
    "print(f'{datetime.now()} [ INFO  ] Finished reading *.metadata filenames from directory. Working with {len(files)} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-11 13:59:43.626937 [ INFO  ] Curently are 1855658 XMLs, 1855658 metadatas,1855658 texts, 1855658 lemmas and 0 event files.\n"
     ]
    }
   ],
   "source": [
    "xml_count = 0\n",
    "metadata_count = 0\n",
    "text_count = 0\n",
    "lemmas_count = 0\n",
    "events_count = 0\n",
    "for dirpath, _, filenames in os.walk(data_path):\n",
    "    xml_count += len([filename for filename in filenames if filename.endswith('.xml')])\n",
    "    metadata_count += len([filename for filename in filenames if filename.endswith('metadata')])\n",
    "    text_count += len([filename for filename in filenames if filename.endswith('.text')])\n",
    "    lemmas_count += len([filename for filename in filenames if filename.endswith('.lemmas.p')])\n",
    "    events_count += len([filename for filename in filenames if filename.endswith('.events.p')])\n",
    "print(f'{datetime.now()} [ INFO  ] Curently are {xml_count} XMLs, {metadata_count} metadatas,{text_count} texts, {lemmas_count} lemmas and {events_count} event files.')\n",
    "\n",
    "if events_count>0:\n",
    "    print(f'{datetime.now()} [WARNING] Some files are goint to be overwritten (*.events.p)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-11 13:59:43.710614 [ INFO  ] Loading BERT class\n",
      "2020-06-11 13:59:43.711439 [ INFO  ] BERT class loaded.\n"
     ]
    }
   ],
   "source": [
    "print(f'{datetime.now()} [ INFO  ] Loading BERT class')\n",
    "class BERT_Embeddings(object):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        # Load pre-trained model (weights)\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def get_BERT_Embeddings(self,text, tokens):\n",
    "\n",
    "            \n",
    "        matrix = np.zeros(shape=(1,len(tokens),768)) # 768 last 4 layer added.\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        tokenized_text = ['[CLS]']\n",
    "        \n",
    "        for idx, (ini,fin) in enumerate(tokens):\n",
    "            word_tokens = self.tokenizer.tokenize(text[ini:fin])\n",
    "\n",
    "            if len(word_tokens)+len(tokenized_text)+1 >512: #if sentence too long exclude some tokens\n",
    "                break \n",
    "                \n",
    "            tokenized_text += word_tokens\n",
    "        \n",
    "        # truncate sentence if is longer than 512 (BERT model can't handle more than 512)\n",
    "        # if sentence too long exclude some tokens (the vectors of those tokens will be zero)\n",
    "        tokens = tokens[:idx]\n",
    "\n",
    "            \n",
    "        tokenized_text += ['[SEP]']\n",
    "\n",
    "            \n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n",
    "            \n",
    "        token_embeddings = [] \n",
    "        layer_i = 0\n",
    "        batch_i = 0\n",
    "        token_i = 0\n",
    "\n",
    "        # For each token in the sentence...\n",
    "        for token_i in range(len(tokenized_text)):\n",
    "            hidden_layers = [] \n",
    "\n",
    "            # For each of the 12 layers...\n",
    "            for layer_i in range(len(encoded_layers)):\n",
    "\n",
    "                # Lookup the vector for `token_i` in `layer_i`\n",
    "                vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "                hidden_layers.append(vec)\n",
    "\n",
    "            token_embeddings.append(hidden_layers)\n",
    "\n",
    "        summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings] # [number_of_tokens, 768]\n",
    "\n",
    "        inicio=1\n",
    "        for idx,token in enumerate(tokens):\n",
    "                ini,fin = token\n",
    "                individual_tokenized = self.tokenizer.tokenize(text[ini:fin])\n",
    "                cantidad_embeddings = len(individual_tokenized)\n",
    "                group_tokenized = tokenized_text[inicio:inicio+cantidad_embeddings]\n",
    "                                \n",
    "                assert all([t1==t2 for t1,t2 in zip(individual_tokenized,group_tokenized)]),  '{} {} {}'.format(\n",
    "                                 text[ini:fin],\n",
    "                                 individual_tokenized,\n",
    "                                 group_tokenized\n",
    "                             )\n",
    "\n",
    "                if cantidad_embeddings>0:\n",
    "                    for embedding in summed_last_4_layers[inicio:inicio+cantidad_embeddings]:\n",
    "                        matrix[0,idx,:]+=embedding.numpy()\n",
    "                    matrix[0,idx,:] = matrix[0,idx,:]/cantidad_embeddings\n",
    "                    inicio+=cantidad_embeddings\n",
    "\n",
    "        return matrix\n",
    "print(f'{datetime.now()} [ INFO  ] BERT class loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from modules.ed import get_data, load_model_from_disk, Generator, train_val_split, train_model, get_model, sensitivity, specificity, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-11 13:59:46.654614 [ INFO  ] Loading data from files (testing_sents.p and training_sents.p) on folder: /mnt/work/maiso/python3.workspace/Causality/data/ed/\n",
      "[  OK   ] Loading training indexes.\n",
      "2020-06-11 14:01:19.055195 [ INFO  ] Loading model from: /mnt/work/maiso/python3.workspace/Causality/models/best_model_15.h5\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /mnt/work/maiso/environments/causality/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2020-06-11 14:01:20.088195 [ INFO  ] evaluating model on validation_data\n",
      "loss: 0.1672\n",
      "acc: 0.9374\n",
      "sensitivity: 0.6883\n",
      "specificity: 0.9411\n",
      "f1_score: 0.6916\n"
     ]
    }
   ],
   "source": [
    "from modules.ed import get_data, load_model_from_disk, Generator, train_val_split, train_model, get_model, sensitivity, specificity, f1_score\n",
    "\n",
    "data_path='/mnt/work/maiso/python3.workspace/Causality/data/ed/'\n",
    "model_path = '/mnt/work/maiso/python3.workspace/Causality/models/best_model_15.h5'\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Loading data from files (testing_sents.p and training_sents.p) on folder: {data_path}')\n",
    "      \n",
    "    \n",
    "x, y, ent2index = get_data(data_path)\n",
    "train_data, val_data = train_val_split((x,y))\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Loading model from: {model_path}')\n",
    "# model = load_model_from_disk(model_path)\n",
    "model = get_model(len(ent2index), hidden_units_count=15)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "          optimizer='adam', \n",
    "          metrics=['accuracy',sensitivity,specificity, f1_score])\n",
    "\n",
    "model.load_weights(model_path)\n",
    "\n",
    "model._make_predict_function() # para read-only y evitar problemas de sincronizaci√≥n con multithreading\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] evaluating model on validation_data')\n",
    "x_val, y_val = val_data\n",
    "rta = model.evaluate(Generator(list(zip(x_val, y_val))), verbose=0)\n",
    "\n",
    "# for metric, value in zip(['loss']+[m.name for m in model.metrics], rta):\n",
    "#     print(f'{datetime.now()} [ INFO  ] {metric:12}: {value}')\n",
    "for metric,value in zip(model.metrics_names, rta):\n",
    "    print('{}: {:5.4f}'.format(metric, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-11 14:48:03.341957 [ INFO  ] Loading BERT model.\n",
      "2020-06-11 14:48:03.342254 [ INFO  ] BERT model loaded.\n",
      "2020-06-11 14:48:03.343246 [ INFO  ] process_file defined. Uses: metadata, lemmas and text for building the event.p files.\n"
     ]
    }
   ],
   "source": [
    "# Inputs: \n",
    "bert = BERT_Embeddings()\n",
    "# bert_lock = threading.Lock()\n",
    "\n",
    "    \n",
    "# model_lock = threading.Lock()\n",
    "tuplas_re = re.compile('\\(([0-9]*?), ([0-9]*?)\\)')\n",
    "entities_re = re.compile(\"'([^']*)'\")\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] Loading BERT model.')\n",
    "print(f'{datetime.now()} [ INFO  ] BERT model loaded.')\n",
    "\n",
    "def process_file(file):\n",
    "\n",
    "\n",
    "\n",
    "    # read file (metadata + text)\n",
    "    text = open(file[:-9]+'.text', 'r' ).read()\n",
    "    lemmas = pickle.load(open(file[:-9]+'.lemmas.p', 'rb' ))\n",
    "\n",
    "    metadata = open(file,'r').read().splitlines()\n",
    "    id_ = metadata[0][4:] #len('ID: ') == 4\n",
    "    date = metadata[1][6:] # len('DATE: ') == 6\n",
    "\n",
    "    # process file\n",
    "    tokens = tuplas_re.findall(metadata[3])\n",
    "    sentences = tuplas_re.findall(metadata[2])\n",
    "\n",
    "    tokens = [(int(ini),int(fin)) for ini, fin in tokens]\n",
    "    sentences = [(int(ini),int(fin)) for ini, fin in sentences]\n",
    "\n",
    "    entities = entities_re.findall(metadata[4])\n",
    "\n",
    "    idx = 0\n",
    "    old_idx = 0\n",
    "    sent_idx=0\n",
    "    events = []\n",
    "\n",
    "    for ini,fin in sentences:\n",
    "        while idx<len(tokens) and tokens[idx][0]<fin:\n",
    "            idx+=1\n",
    "        tokens_in_sentence = tokens[old_idx:idx]\n",
    "        lemmas_in_sentence = lemmas[old_idx:idx]\n",
    "        entities_in_sentence = entities[old_idx:idx]\n",
    "        old_idx = idx\n",
    "\n",
    "\n",
    "        xbert = bert.get_BERT_Embeddings(text, tokens_in_sentence )\n",
    "\n",
    "        xsentbert = np.average(xbert, axis=1) *np.ones(shape=(1,len(xbert[0,:,0]),768))\n",
    "        xents = np.zeros(shape=(1,len(tokens_in_sentence)))\n",
    "\n",
    "\n",
    "        for idx,entity in enumerate(entities_in_sentence):\n",
    "            if entity in ent2index:\n",
    "                xents[0,idx] = ent2index[entity]\n",
    "            else:\n",
    "                xents[0,idx] = ent2index['[PAD]']\n",
    "\n",
    "        x = [xbert, xsentbert, xents]\n",
    "\n",
    "        y_pred = model.predict(x)\n",
    "\n",
    "        for token, pred in zip(tokens_in_sentence, y_pred[0,:,0]):\n",
    "            if pred>0.5:\n",
    "                print(f'event: {text[token[0]:token[1]]} - conf: {pred} - sent:{sent_idx}')\n",
    "    \n",
    "    \n",
    "        for j in range(len(tokens_in_sentence)):\n",
    "            pred = y_pred[0,j,0]\n",
    "            if pred>0.5:\n",
    "                bert_vec = xbert[0,j,:]\n",
    "                token = tokens_in_sentence[j]\n",
    "\n",
    "\n",
    "                event = {\n",
    "                    'file_id':id_,\n",
    "                    'trigger': text[token[0]:token[1]],\n",
    "                    'idx': sent_idx,\n",
    "                    'bert': bert_vec,\n",
    "                    'conf': pred,\n",
    "                    'lemma': lemmas_in_sentence[j],\n",
    "                    'date': date\n",
    "                }\n",
    "                events.append(event)\n",
    "        sent_idx+=1\n",
    "    return events\n",
    "#    pickle.dump(events ,open(file[:-9]+'.events.p', 'wb' ))\n",
    "\n",
    "\n",
    "\n",
    "print(f'{datetime.now()} [ INFO  ] process_file defined. Uses: metadata, lemmas and text for building the event.p files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event: LEAD - conf: 0.676846444606781 - sent:0\n",
      "event: * - conf: 0.6415690779685974 - sent:2\n",
      "event: * - conf: 0.6415690779685974 - sent:3\n",
      "event: * - conf: 0.6840687394142151 - sent:4\n",
      "event: * - conf: 0.6415690779685974 - sent:5\n",
      "event: sales - conf: 0.6840687394142151 - sent:20\n",
      "event: * - conf: 0.6415690779685974 - sent:31\n",
      "event: * - conf: 0.6415690779685974 - sent:32\n",
      "event: * - conf: 0.6840687394142151 - sent:33\n",
      "event: * - conf: 0.6415690779685974 - sent:34\n",
      "event: sales - conf: 0.6840687394142151 - sent:49\n",
      "event: said - conf: 0.6840687394142151 - sent:61\n",
      "event: outstanding - conf: 0.6840687394142151 - sent:61\n",
      "event: offering - conf: 0.6840687394142151 - sent:61\n",
      "file_id: 0000000 - trigger: LEAD - idx: 0 - lemma: lead - conf: 0.676846444606781 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: * - idx: 2 - lemma: * - conf: 0.6415690779685974 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: * - idx: 3 - lemma: * - conf: 0.6415690779685974 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: * - idx: 4 - lemma: * - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: * - idx: 5 - lemma: * - conf: 0.6415690779685974 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: sales - idx: 20 - lemma: sale - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: * - idx: 31 - lemma: * - conf: 0.6415690779685974 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: * - idx: 32 - lemma: * - conf: 0.6415690779685974 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: * - idx: 33 - lemma: * - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: * - idx: 34 - lemma: * - conf: 0.6415690779685974 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: sales - idx: 49 - lemma: sale - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: said - idx: 61 - lemma: say - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: outstanding - idx: 61 - lemma: outstanding - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: offering - idx: 61 - lemma: offering - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "file = files[0]\n",
    "\n",
    "events = process_file(file)\n",
    "\n",
    "\n",
    "# text = open(file[:-9]+'.text', 'r' ).read()\n",
    "# doc = nlp(text)\n",
    "# for idx,sentence in enumerate(doc.sents):\n",
    "#     print(f'{idx}: {sentence}')\n",
    "    \n",
    "for event in events:\n",
    "    print(f\"file_id: {event['file_id']} - trigger: {event['trigger']} - idx: {event['idx']} - lemma: {event['lemma']} - conf: {event['conf']} - date: {event['date']}\")\n",
    "    print('-----')\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LEAD:\n",
      "*3*** COMPANY REPORTS **\n",
      "*3*AAR CORP (NYSE)\n",
      "Qtr to Nov 30\n",
      "1986\n",
      "1985\n",
      "Sales\n",
      "75,907,000\n",
      "61,040,000\n",
      "Net inc\n",
      "3,953,000\n",
      "2,858,000\n",
      "Share earns\n",
      ".38\n",
      ".32\n",
      "Shares outst\n",
      "10,479,000\n",
      "9,069,000\n",
      "6mo sales\n",
      "142,283,000\n",
      "114,876,000\n",
      "Net inc\n",
      "7,054,000\n",
      "5,300,000\n",
      "Share earns\n",
      ".71\n",
      ".59\n",
      "Shares outst\n",
      "9,932,000\n",
      "9,066,000\n",
      "*3*** COMPANY REPORTS **\n",
      "*3*AAR CORP (NYSE)\n",
      "Qtr to Nov 30\n",
      "1986\n",
      "1985\n",
      "Sales\n",
      "75,907,000\n",
      "61,040,000\n",
      "Net inc\n",
      "3,953,000\n",
      "2,858,000\n",
      "Share earns\n",
      ".38\n",
      ".32\n",
      "Shares outst\n",
      "10,479,000\n",
      "9,069,000\n",
      "6mo sales\n",
      "142,283,000\n",
      "114,876,000\n",
      "Net inc\n",
      "7,054,000\n",
      "5,300,000\n",
      "Share earns\n",
      ".71\n",
      ".59\n",
      "Shares outst\n",
      "9,932,000\n",
      "9,066,000\n",
      "The 1985 share earnings and shares outstanding are adjusted for the 3-for- 2 stock split\n",
      "          in February 1986.\n",
      "The company said the 1986 shares outstanding reflects the pro rata effect of the issuance\n",
      "          of 1.35 million shares in public offering in August 1986.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open(files[0][:-9]+'.text', 'r' ).read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "LEAD:\n",
      "\n",
      "1: *3\n",
      "2: *\n",
      "3: *\n",
      "4: * COMPANY REPORTS\n",
      "5: *\n",
      "6: *\n",
      "\n",
      "7: *3*AAR CORP (NYSE)\n",
      "\n",
      "8: Qtr to Nov 30\n",
      "1986\n",
      "\n",
      "9: 1985\n",
      "\n",
      "10: Sales\n",
      "75,907,000\n",
      "\n",
      "11: 61,040,000\n",
      "\n",
      "12: Net inc\n",
      "\n",
      "13: 3,953,000\n",
      "\n",
      "14: 2,858,000\n",
      "\n",
      "15: Share earns\n",
      "\n",
      "16: .38\n",
      "\n",
      "17: .32\n",
      "\n",
      "18: Shares outst\n",
      "10,479,000\n",
      "\n",
      "19: 9,069,000\n",
      "\n",
      "20: 6mo sales\n",
      "\n",
      "21: 142,283,000\n",
      "\n",
      "22: 114,876,000\n",
      "\n",
      "23: Net inc\n",
      "\n",
      "24: 7,054,000\n",
      "\n",
      "25: 5,300,000\n",
      "\n",
      "26: Share earns\n",
      "\n",
      "27: .71\n",
      "\n",
      "28: .59\n",
      "\n",
      "29: Shares outst\n",
      "9,932,000\n",
      "9,066,000\n",
      "\n",
      "30: *3\n",
      "31: *\n",
      "32: *\n",
      "33: * COMPANY REPORTS\n",
      "34: *\n",
      "35: *\n",
      "\n",
      "36: *3*AAR CORP (NYSE)\n",
      "\n",
      "37: Qtr to Nov 30\n",
      "1986\n",
      "\n",
      "38: 1985\n",
      "\n",
      "39: Sales\n",
      "75,907,000\n",
      "\n",
      "40: 61,040,000\n",
      "\n",
      "41: Net inc\n",
      "\n",
      "42: 3,953,000\n",
      "\n",
      "43: 2,858,000\n",
      "\n",
      "44: Share earns\n",
      "\n",
      "45: .38\n",
      "\n",
      "46: .32\n",
      "\n",
      "47: Shares outst\n",
      "10,479,000\n",
      "\n",
      "48: 9,069,000\n",
      "\n",
      "49: 6mo sales\n",
      "\n",
      "50: 142,283,000\n",
      "\n",
      "51: 114,876,000\n",
      "\n",
      "52: Net inc\n",
      "\n",
      "53: 7,054,000\n",
      "\n",
      "54: 5,300,000\n",
      "\n",
      "55: Share earns\n",
      "\n",
      "56: .71\n",
      "\n",
      "57: .59\n",
      "\n",
      "58: Shares outst\n",
      "9,932,000\n",
      "\n",
      "59: 9,066,000\n",
      "\n",
      "60: The 1985 share earnings and shares outstanding are adjusted for the 3-for- 2 stock split\n",
      "          in February 1986.\n",
      "\n",
      "61: The company said the 1986 shares outstanding reflects the pro rata effect of the issuance\n",
      "          of 1.35 million shares in public offering in August 1986.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx,sentence in enumerate(doc.sents):\n",
    "    print(f'{idx}: {sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_id: 0000000 - trigger: said - idx: 61 - lemma: say - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: outstanding - idx: 61 - lemma: outstanding - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n",
      "file_id: 0000000 - trigger: offering - idx: 61 - lemma: offering - conf: 0.6840687394142151 - date: 19870101\n",
      "-----\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for event in events:\n",
    "    print(f\"file_id: {event['file_id']} - trigger: {event['trigger']} - idx: {event['idx']} - lemma: {event['lemma']} - conf: {event['conf']} - date: {event['date']}\")\n",
    "    print('-----')\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_id', 'trigger', 'idx', 'bert', 'conf', 'lemma', 'date'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68406874"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[0]['conf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
